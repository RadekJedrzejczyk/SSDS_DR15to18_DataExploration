{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf66472-fef5-4f5e-a293-7d536f32711f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:45:17.812688Z",
     "start_time": "2025-01-09T14:45:15.392334Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a5764b-4dbd-4388-a63e-c637a6c5b44c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:45:20.128088Z",
     "start_time": "2025-01-09T14:45:17.814826Z"
    }
   },
   "outputs": [],
   "source": [
    "dr14 = pd.read_csv('Dataset/SDSS_DR14.csv')\n",
    "dr16 = pd.read_csv('Dataset/SDSS_DR16.csv')\n",
    "dr17 = pd.read_csv('Dataset/SDSS_DR17.csv')\n",
    "dr18 = pd.read_csv('Dataset/SDSS_DR18.csv') \n",
    "dataframe = pd.concat([dr14, dr16, dr17, dr18], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c265b4a-04bf-45d2-9211-c976bebe0bea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:45:20.817480Z",
     "start_time": "2025-01-09T14:45:20.130094Z"
    }
   },
   "outputs": [],
   "source": [
    "df = dataframe.dropna(axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca7659-8b86-444b-9b99-59ff76105872",
   "metadata": {},
   "source": [
    "I need to get rid of values that are not correlated in any way to our class - those are going to be technical informations about observatory, ID numbers etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268d616-0bdf-4390-a82a-db593fc647fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:45:20.859638Z",
     "start_time": "2025-01-09T14:45:20.820490Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df.drop(columns=['objid','run','rerun','camcol','field','specobjid','plate','mjd','fiberid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8587bcb-9e9e-4f82-b178-e113ea958667",
   "metadata": {},
   "source": [
    "Dodatkowo pozbędziemy się fałszywych danych poza zakresami "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a95f9d-f877-474d-9e3f-7c0b10dd9226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:45:21.423706Z",
     "start_time": "2025-01-09T14:45:20.862648Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "5c279f01",
=======
   "id": "cc6b98d7",

   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['class'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "ba507d85-e8c6-4b5a-83cc-37b7f40519db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "uniques = pd.unique(df['class'])\n",
    "binary_map = {}\n",
    "for number, value in enumerate(uniques):\n",
    "    binary_map[value] = number\n",
    "df['class'] = df['class'].replace(binary_map)\n",
    "binary_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
>>>>>>> wzorce-sekwencji
   "id": "c4c0dfa9-12b0-4fdb-9439-a122d4faa133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:45:44.811735Z",
     "start_time": "2025-01-09T14:45:21.612464Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in df.select_dtypes(include=['number']).columns:  # Ensure numerical data\n",
    "    plt.figure()  # Create a new figure for each histogram\n",
    "    sns.histplot(df[column], kde=True, bins=10, color='blue')  # Histogram with KDE\n",
    "    plt.title(f'Histogram for {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d21ba8-eea3-48b6-b44c-c668138fb225",
   "metadata": {},
   "source": [
    "Wybieramy zakresy dla poszczególnych kolumn a następnie filtrujemy dane - ryzykujemy utratę niektórych przypadków, ale ewidentnie w zakresie danych są takie które są fizycznie niemożliwe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d153b71-42de-440f-bd3c-29b042cacec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:21.220436Z",
     "start_time": "2025-01-09T14:45:44.813745Z"
    }
   },
   "outputs": [],
   "source": [
    "ranges={\n",
    "\"u\": (-2000,50),\n",
    "\"g\": (-2000,50),\n",
    "\"r\": (0,50),\n",
    "\"z\": (-2000,50),\n",
    "}\n",
    "for column, (min_val, max_val) in ranges.items():\n",
    "    df = df[(df[column] >= min_val) & (df[column] <= max_val)]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for column in df.select_dtypes(include=['number']).columns:  # Ensure numerical data\n",
    "    plt.figure()  # Create a new figure for each histogram\n",
    "    sns.histplot(df[column], kde=True, bins=10, color='blue')  # Histogram with KDE\n",
    "    plt.title(f'Histogram for {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397ad9c-414e-4053-8758-d0766c9b7331",
   "metadata": {},
   "source": [
    "Podzielmy dane na dwi oddzielne zestawy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba64c0-fdec-4950-8269-551d363a76e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:21.247628Z",
     "start_time": "2025-01-09T14:46:21.222445Z"
    }
   },
   "outputs": [],
   "source": [
    "y=df['class']\n",
    "x=df.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1feb9f8-b756-489d-82be-d74978ab03cf",
   "metadata": {},
   "source": [
    "Przeprowadzimy teraz \n",
    "## Analizę składowych głównych\n",
    "zaczniemy od standaryzacji zmiennych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f638ec5-fb2d-44d7-b85f-c761dc78eeba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:21.423562Z",
     "start_time": "2025-01-09T14:46:21.250673Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_df = x.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876f085-abea-444e-92ea-60c7b8092032",
   "metadata": {},
   "source": [
    "Teraz przejdziemy do właściwej analizy sprawdzimy dwie rzeczy:\n",
    "- jak liczba składowych wpływa na całkowitą wariancję\n",
    "- następnie wybierzemy jedną konkretną liczbę składowych głownych i obliczymy składowe główne dla niej\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e8c1d-89f2-4143-bf11-b6579e290336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:21.971005Z",
     "start_time": "2025-01-09T14:46:21.424592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tworzenie obiektu PCA, gdzie nie określasz liczby komponentów\n",
    "pca = PCA()\n",
    "\n",
    "# Dopasowanie i transformacja danych\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Sprawdzenie skumulowanej wariancji wyjaśnianej przez kolejne komponenty\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f\"Skumulowana wariancja: {cumulative_variance}\")\n",
    "\n",
    "# Wykres skumulowanej wariancji\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Liczba głównych składowych')\n",
    "plt.ylabel('Skumulowana wyjaśniona wariancja')\n",
    "plt.title('Wykres skumulowanej wyjaśnionej wariancji')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b67aad-bb28-4a98-8af0-3336602430f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:22.050938Z",
     "start_time": "2025-01-09T14:46:21.973013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie obiektu PCA \n",
    "n_components=4 #liczba składowych głównych\n",
    "pca = PCA(n_components)\n",
    "\n",
    "# Dopasowanie i transformacja danych\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Sprawdzenie ile wariancji wyjaśniają komponenty\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Wariancja wyjaśniana przez każdą główną składową: {explained_variance}\")\n",
    "\n",
    "# Utworzenie DataFrame z wynikami analizy PCA (ze zredukowanymi wymiarami)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Uzyskanie macierzy składowych\n",
    "components = pca.components_\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "components_df = pd.DataFrame(components, columns=x.select_dtypes(include=[np.number]).columns, index=[f'PC{i+1}' for i in range(components.shape[0])])\n",
    "\n",
    "# Wyświetlenie macierzy składowych\n",
    "print(components_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62870379-cabc-4970-b482-ccaad353fe0e",
   "metadata": {},
   "source": [
    "Widzimy, że wartość dla ra i dec w pierwszej składowej są bardzo małe, natomiast składowe które je wyjaśniają (2 i 3) bardzo słabo wyjaśniają pozostałe. Sprawdźmy na wszelki wypadek macierz korelacji, bo być może ra i dec są wgl niepotrzebne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ee8b9-488b-47cc-ba9d-61a0f46b6b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:23.075078Z",
     "start_time": "2025-01-09T14:46:22.054023Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "correlation = pd.concat([x,y],axis=1)\n",
    "sns.heatmap(x.corr(), annot=True,cmap='jet',fmt='.2f') #jet,copper, coolwarm\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8e9b1-2a1b-4d32-8b9e-416e4eb88c1b",
   "metadata": {},
   "source": [
    "Wartości ra i dec praktycznie wgl nie pomagają nam w klasyfikacji, pozbędziemy się ich i jeszcz raz ustalimy składowe główne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fda87-3ae5-491a-bdeb-41162dd3767f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:23.385732Z",
     "start_time": "2025-01-09T14:46:23.079087Z"
    }
   },
   "outputs": [],
   "source": [
    "x=x.loc[:, x.columns != 'ra']\n",
    "x=x.loc[:, x.columns != 'dec']\n",
    "numeric_df = x.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_df)\n",
    "# Tworzenie obiektu PCA, gdzie nie określasz liczby komponentów\n",
    "pca = PCA()\n",
    "\n",
    "# Dopasowanie i transformacja danych\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Sprawdzenie skumulowanej wariancji wyjaśnianej przez kolejne komponenty\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f\"Skumulowana wariancja: {cumulative_variance}\")\n",
    "\n",
    "# Wykres skumulowanej wariancji\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(cumulative_variance)\n",
    "plt.xlabel('Liczba głównych składowych')\n",
    "plt.ylabel('Skumulowana wyjaśniona wariancja')\n",
    "plt.title('Wykres skumulowanej wyjaśnionej wariancji')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdc842-d097-4d1a-bc49-e87125ca6271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T14:46:23.432674Z",
     "start_time": "2025-01-09T14:46:23.387739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie obiektu PCA \n",
    "n_components=2 #liczba składowych głównych\n",
    "pca = PCA(n_components)\n",
    "\n",
    "# Dopasowanie i transformacja danych\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Sprawdzenie ile wariancji wyjaśniają komponenty\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Wariancja wyjaśniana przez każdą główną składową: {explained_variance}\")\n",
    "\n",
    "# Utworzenie DataFrame z wynikami analizy PCA (ze zredukowanymi wymiarami)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Uzyskanie macierzy składowych\n",
    "components = pca.components_\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "components_df = pd.DataFrame(components, columns=x.select_dtypes(include=[np.number]).columns, index=[f'PC{i+1}' for i in range(components.shape[0])])\n",
    "\n",
    "# Wyświetlenie macierzy składowych\n",
    "print(components_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c806d3-ef52-4e78-8427-45e5a7fd211b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:36:16.006555Z",
     "start_time": "2025-01-09T15:36:15.970273Z"
    }
   },
   "outputs": [],
   "source": [
    "PCA_values =   x.values @ components_df.T.values\n",
    "df_PCA_x = pd.DataFrame(PCA_values,columns=[f'PC{i+1}' for i in range(components.shape[0])])\n",
    "#df_PCA=pd.concat([df_PCA_x,y],axis=1)\n",
    "df_PCA = df_PCA_x\n",
    "df_PCA['class'] = pd.DataFrame(y)\n",
    "df_PCA\n",
    "df_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbd033-cc6a-4c15-9366-49b854ce8d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:36:40.773362Z",
     "start_time": "2025-01-09T15:36:16.729959Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='PC1',y='PC2',hue='class',data=df_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "7490545a",
=======
   "id": "b7801a87-736a-471d-a14b-19d0faca8304",
   "metadata": {},
   "source": [
    "Alternatywnie moglibyśmy również zmaksymalizować dokładność poprzez dodanie jeszcze jednej składowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b930a5-e14a-414b-a390-29f8a261b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie obiektu PCA \n",
    "n_components=3 #liczba składowych głównych\n",
    "pca = PCA(n_components)\n",
    "\n",
    "# Dopasowanie i transformacja danych\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Sprawdzenie ile wariancji wyjaśniają komponenty\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Wariancja wyjaśniana przez każdą główną składową: {explained_variance}\")\n",
    "\n",
    "# Utworzenie DataFrame z wynikami analizy PCA (ze zredukowanymi wymiarami)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Uzyskanie macierzy składowych\n",
    "components = pca.components_\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "components_df = pd.DataFrame(components, columns=x.select_dtypes(include=[np.number]).columns, index=[f'PC{i+1}' for i in range(components.shape[0])])\n",
    "\n",
    "# Wyświetlenie macierzy składowych\n",
    "print(components_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a5b06-ecdd-4d52-a449-8a79e1c6e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_values =   x.values @ components_df.T.values\n",
    "df_PCA_x = pd.DataFrame(PCA_values,columns=[f'PC{i+1}' for i in range(components.shape[0])])\n",
    "#df_PCA=pd.concat([df_PCA_x,y],axis=1)\n",
    "df_PCA_alternative = df_PCA_x\n",
    "df_PCA_alternative['class'] = pd.DataFrame(y)\n",
    "df_PCA_alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e2de9-d862-4d80-ad43-d4df593c74fe",
   "metadata": {},
   "source": [
    "Wyświetlimy wykres interaktywnie, w celu poprawy wydajności pobierzemy więc próbkę naszych punktów (~1/3 całości) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4d669-cda5-48db-abd3-c53d7ae465d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df_PCA_alternative_sample=df_PCA_alternative.sample(100000, random_state=32)\n",
    "fig=px.scatter_3d(df_PCA_alternative_sample,x='PC1',y='PC2',z='PC3',color='class' )\n",
    "fig.update_traces(marker={'size': 1})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db470a7-d759-417e-8900-45dc6e2e29bd",
   "metadata": {},
   "source": [
    "Dodanie dodatkowego wymiaru uwypukla poprzednio już widoczną różnicę między QSO a innymi obiektami. Dodatkowo nieco wyraźniejsza staje się różnica między gwiazdami i galaktymi - te drugie przyjmują bliższe są ujemnych wartości dla PC3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a77568",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h2 align = 'center'> Klasteryzacja </h2>\n",
    "<h3 align = 'center'> KMeans </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab8182",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Normalizacja danych - przygotowanie do klasteryzacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "15cb993c",
=======
   "id": "001aa387-fd3d-492d-ba82-e71f25fd8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "uniques = pd.unique(df['class'])\n",
    "binary_map = {}\n",
    "for number, value in enumerate(uniques):\n",
    "    binary_map[value] = number\n",
    "df['class'] = df['class'].replace(binary_map)\n",
    "binary_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f24c8",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_class = df['class']\n",
    "df = df.drop('class',axis=1)\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "df['class'] = df_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff368a3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Przed normalizacją usuneliśmy z danych klase obiektu żeby nie mała ona wpływu na dopasowanie do klasteru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e77c9d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Klasteryzacja z użyciem KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "79354200",
=======
   "id": "3d7f70a2",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(data_scaled)\n",
    "df['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63e7a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Metoda łokcia"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "1c3191bd",
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e79e5",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
<<<<<<< HEAD
   "source": [
    "Metoda łokcia to technika stosowana w analizie skupień (clustering) w celu określenia optymalnej liczby klastrów w algorytmie grupowania, takim jak k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885078de",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
>>>>>>> wzorce-sekwencji
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    kmeans_ml = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_ml.fit(data_scaled)\n",
    "    inertias.append(kmeans_ml.inertia_)\n",
    "plt.plot(range(1, 11), inertias, marker='o')\n",
    "plt.title(\"Metoda Łokcia\")\n",
    "plt.xlabel(\"Liczba klastrów\")\n",
    "plt.ylabel(\"Inercja\")\n",
    "plt.show()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "926e5b77",
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0f143",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
<<<<<<< HEAD
   "source": [
    "W naszym przypadku zdecydowaliśmy się na 3 klastry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3907957",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Rozkłąd danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4017ed3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
>>>>>>> wzorce-sekwencji
   "outputs": [],
   "source": [
    "df.groupby('Cluster').mean()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "9cc31b6c",
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "60b97e7e",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
<<<<<<< HEAD
   "source": [
    "W powyższej tabeli widzimy srednią wartość poszczególnych kolumn dla każdego z klastrów\n",
    "Poniżej znajduje liczba elementów przypadających na dany klaster i klase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233ed91",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
>>>>>>> wzorce-sekwencji
   "outputs": [],
   "source": [
    "df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "8bdc5d72",
=======
   "id": "d2d58782",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "ccda99dd",
=======
   "id": "ee21fdec",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(pd.crosstab(df['class'], df['Cluster']), annot=True, linewidths=0.5, fmt='d')\n",
    "plt.title(\"Contingency Table Heatmap: Class vs Cluster\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "805fc569",
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0858a",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
<<<<<<< HEAD
   "source": [
    "Reprezentacja ilości elementów danej klasy w danym klastrze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7775b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
>>>>>>> wzorce-sekwencji
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df['class'], df['Cluster'])\n",
    "contingency_percentage =  contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(contingency_percentage, annot=True, linewidths=0.5, fmt='f')\n",
    "plt.title(\"Contingency Table Heatmap: Class vs Cluster\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "ee9c00e9",
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3073d",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
<<<<<<< HEAD
   "source": [
    "Reprezentacja rozkładu elementów klas na klastry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2b884",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Poniżej znajduje sie kilka wykresów przedstawiających wizualizacje podziału"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0300c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
>>>>>>> wzorce-sekwencji
   "outputs": [],
   "source": [
    "feature_1 = df['z']\n",
    "feature_2 = df['g']\n",
    "feature_3 = df['u']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(feature_1, feature_2, feature_3, c=clusters, cmap='viridis', s=1, alpha=0.2)\n",
    "ax.set_title(\"Wizualizacja klasteryzacji w 3D\")\n",
    "ax.set_xlabel(\"Cecha 1\")\n",
    "ax.set_ylabel(\"Cecha 2\")\n",
    "ax.set_zlabel(\"Cecha 3\")\n",
    "ax.view_init(elev=40, azim=240)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "43fa8bda",
=======
   "id": "f1685e86",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "feature_1 = df['ra']\n",
    "feature_2 = df['dec']\n",
    "feature_3 = df['u']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(feature_1, feature_2, feature_3, c=clusters, cmap='viridis', s=1, alpha=0.2)\n",
    "ax.set_title(\"Wizualizacja klasteryzacji w 3D\")\n",
    "ax.set_xlabel(\"Cecha 1\")\n",
    "ax.set_ylabel(\"Cecha 2\")\n",
    "ax.set_zlabel(\"Cecha 3\")\n",
    "ax.view_init(elev=40, azim=240)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "d99814d3",
=======
   "id": "338384d4",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "feature_1 = df['redshift']\n",
    "feature_2 = df['class']\n",
    "feature_3 = df['u']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(feature_1, feature_2, feature_3, c=clusters, cmap='viridis', s=1, alpha=0.2)\n",
    "ax.set_title(\"Wizualizacja klasteryzacji w 3D\")\n",
    "ax.set_xlabel(\"Cecha 1\")\n",
    "ax.set_ylabel(\"Cecha 2\")\n",
    "ax.set_zlabel(\"Cecha 3\")\n",
    "ax.view_init(elev=40, azim=240)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cbaa1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h3 align = 'center'> KMeans na PCA </h3>"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "a54f125a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "c7043dc1-df49-4a13-b34a-91a376854253",
   "metadata": {},
   "outputs": [],
>>>>>>> wzorce-sekwencji
   "source": [
    "Dla prorównania przeprowadzamy klsateryzacje na danych które zostały poddane redukcji wymiarów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "024b6bef",
=======
   "id": "e097fcf6",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scaler_pca = StandardScaler()\n",
    "df_PCA_class = df_PCA['class']\n",
    "df_PCA = df_PCA.drop('class',axis=1)\n",
    "data_scaled_pca = scaler.fit_transform(df_PCA)\n",
    "df_PCA['class'] = df_PCA_class\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=42)\n",
    "clusters_pca = kmeans_pca.fit_predict(data_scaled_pca)\n",
    "df_PCA['Cluster'] = clusters_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a251bac4",
=======
   "id": "66e5c8a3",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    kmeans_ml = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_ml.fit(data_scaled_pca)\n",
    "    inertias.append(kmeans_ml.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertias, marker='o')\n",
    "plt.title(\"Metoda Łokcia\")\n",
    "plt.xlabel(\"Liczba klastrów\")\n",
    "plt.ylabel(\"Inercja\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "cd287c14",
=======
   "id": "b8f85408",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_summary = df_PCA.groupby('Cluster').mean()\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "ae74e3f8",
=======
   "id": "9fb4445a",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df_PCA['Cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "176056de",
=======
   "id": "156f11ed",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df_PCA['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "7f2473ed",
=======
   "id": "1148007b",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(pd.crosstab(df_PCA['class'], df_PCA['Cluster']), annot=True, linewidths=0.5, fmt='d')\n",
    "plt.title(\"Contingency Table Heatmap: Class vs Cluster\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "af4301b9",
=======
   "id": "359fcf00",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df_PCA['class'], df_PCA['Cluster'])\n",
    "contingency_percentage =  contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(contingency_percentage, annot=True, linewidths=0.5, fmt='f')\n",
    "plt.title(\"Contingency Table Heatmap: Class vs Cluster\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "dc28461f",
=======
   "id": "e2652e04",
>>>>>>> wzorce-sekwencji
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "feature_1 = df_PCA['PC1']\n",
    "feature_2 = df_PCA['PC2']\n",
    "feature_3 = df_PCA['class']\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(feature_1, feature_2, feature_3, c=clusters, cmap='viridis', s=1, alpha=0.2)\n",
    "ax.set_title(\"Wizualizacja klasteryzacji w 3D\")\n",
    "ax.set_xlabel(\"Cecha 1\")\n",
    "ax.set_ylabel(\"Cecha 2\")\n",
    "ax.set_zlabel(\"Cecha 3\")\n",
    "ax.view_init(elev=40, azim=240)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc67d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
<<<<<<< HEAD
   "outputs": [],
   "source": []
=======
   "source": [
    "<h3 align = 'center'> Analiza koszykowa/reguły asocjacyjne </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c0e15-636b-4991-a80d-b1300ef55304",
   "metadata": {},
   "source": [
    "odnawiamy opis klas z liczb na słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a1c7a-4c05-4138-a1cc-35ce5c322e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826fe5f-e3ae-4294-bcf0-03b93badadbb",
   "metadata": {},
   "source": [
    "Spójrzmy na zakresy w jakich znajdują się nasze dane, aby uzyskać intuicyjne pojęcie na temat tego, czego będziemy szukać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb40440-86bc-4426-85fb-13117a97fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranges(df, features, target):\n",
    "    rules = {}\n",
    "    for cls in df[target].unique():\n",
    "        class_data = df[df[target] == cls]\n",
    "        rules[cls] = {}\n",
    "        for feature in features:\n",
    "            rules[cls][feature] = {\n",
    "                \"min\": class_data[feature].min(),\n",
    "                \"max\": class_data[feature].max()\n",
    "            }\n",
    "    return rules\n",
    "\n",
    "features = [\"u\", \"g\", \"r\",\"z\",\"i\",\"redshift\"]\n",
    "rules = calculate_ranges(df, features, target=\"class\")\n",
    "\n",
    "for cls, cls_rules in rules.items():\n",
    "    print(f\"Zakresy dla klasy '{cls}':\")\n",
    "    for feature, bounds in cls_rules.items():\n",
    "        print(f\"  {feature}: {bounds['min']} <= x <= {bounds['max']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748a060-2014-44fe-be29-6512e281d22e",
   "metadata": {},
   "source": [
    "Aby przeprowadzić analizę koszykową musimy przejść ze zmiennych ciągłych na dyskretne. Wyznaczymy odpowiednie zakresy. Jak wspomniano poprzednio wartości u, g, r, z, i to zasadniczo jasności w różnych spektrach światła, dlatego podzielimy je na identyczne zakresy:\n",
    "-  $\\infty > x > 22$, blade (dim)\n",
    "-  $20 >= x > 17$, średnie (medium)\n",
    "- $17 >= x > - \\infty $, jasne (bright)\n",
    "\n",
    "W przypadku przesunięcia ku czerwieni wybierzemy wartości:\n",
    "- $x<0.05 $, niskie\n",
    "- $0.05<=x<1.5$, średnie\n",
    "- $  2<=x$, wysokie\n",
    "\n",
    "Wybór zakresów jest orientacyjny, ich większa ilość zapewne zwiększyłaby dokładność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a11f9-eefb-4ea8-95a4-6e5be0c7f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = x.copy()\n",
    "x_copy.drop([\"Cluster\",'ra','dec'],axis=1,inplace=True)\n",
    "medium_upperbound = 22\n",
    "bright_upperbound = 17\n",
    "low_redshift_upperbound = 0.05\n",
    "medium_redshift_upperbound = 1.5\n",
    "\n",
    "for column in x_copy:   \n",
    "    if column!=\"redshift\":\n",
    "        name = \"dim_\"+column\n",
    "        x_copy[name] = x_copy[column] > medium_upperbound\n",
    "        name = \"medium_\"+column\n",
    "        x_copy[name] = (x_copy[column] <= medium_upperbound) &   (x_copy[column] > bright_upperbound)\n",
    "        name= \"bright_\"+column\n",
    "        x_copy[name] =x_copy[column] <= bright_upperbound\n",
    "    else:\n",
    "        name=\"low_\"+column\n",
    "        x_copy[name]=x_copy[column]<low_redshift_upperbound\n",
    "        name=\"medium_\"+column\n",
    "        x_copy[name]=(x_copy[column]>=low_redshift_upperbound) & (x_copy[column]<medium_redshift_upperbound)\n",
    "        name=\"high_\"+column\n",
    "        x_copy[name]=x_copy[column]>=medium_redshift_upperbound\n",
    "x_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c8f2d-8a31-46bc-8784-4ee351ad4574",
   "metadata": {},
   "source": [
    "Zmapujemy również rodzaj naszego obiektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f720f-8f79-4ef4-b8ee-cb80b245b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df.copy()\n",
    "x_copy['class'] = pd.DataFrame(y)\n",
    "x_copy.drop(features,axis=1,inplace=True)\n",
    "\n",
    "x_copy['is_Star']=x_copy['class']=='STAR'\n",
    "x_copy['is_Galaxy']=x_copy['class']=='GALAXY'\n",
    "x_copy['is_QSO']=x_copy['class']=='QSO'\n",
    "x_copy.drop('class',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b48554-6754-4ddf-89ee-edccc11f0965",
   "metadata": {},
   "source": [
    "Następnie odnajdziemy reguły wskazujące na rodzaj naszego obiektu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a0b86-72a1-492e-93a3-2d1b4c450061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Używamy apriori do znalezienia częstych zbiorów\n",
    "frequent_itemsets = apriori(x_copy, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generowanie reguł asocjacyjnych\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05,num_itemsets=len(x_copy))\n",
    "\n",
    "# Filtrujemy reguły, które odnoszą się do kolumny 'class'\n",
    "rules['consequents'] = rules['consequents'].apply(lambda x: list(x)[0])  # Zamieniamy krotki na elementy\n",
    "star_rules=rules[rules['consequents']=='is_Star']\n",
    "galaxy_rules =rules[rules['consequents']=='is_Galaxy']\n",
    "qso_rules =rules[rules['consequents']=='is_QSO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc5d9e-49f4-41e4-82fb-d2a2c1ce53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db2593-ddf2-4931-8de2-4433bceb3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94670a9f-cf7b-41a9-a0d4-a7488447bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qso_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c09a8e-0e6a-4e20-ad3e-b1172a6e711c",
   "metadata": {},
   "source": [
    "Dla każdego zestawu odnajdziemy najwyższe możliwe wartości pewności ('confidence') - które powie nam w jakiej części wypadków gdy wystąpi dany zestaw cech to analizowany obiekt jest danego typu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24e429-9bd8-4445-b840-c387f8ba83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(star_rules.loc[star_rules['confidence'].idxmax()]['antecedents'])\n",
    "star_rules['confidence'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1452fd-5705-453c-ab38-d0072d57638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(galaxy_rules.loc[galaxy_rules['confidence'].idxmax()]['antecedents'])\n",
    "galaxy_rules['confidence'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c29be3-4031-41db-ad21-29ea5cbb13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qso_rules.loc[qso_rules['confidence'].idxmax()]['antecedents'])\n",
    "qso_rules['confidence'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4199f0-1827-4785-9c19-72e9b7ccaf58",
   "metadata": {},
   "source": [
    "Porównując dane zauważyć możemy pewne charakterystyczne cechy np.\n",
    "- Wyraźny rozdział widoczny jest w przesunięciu ku czerwieni. Gwiazdy znajdujące się najbliżej mają niską wartość, galaktyki średnią, a obiekty QSO wysoką. Ma to sens z fizycznego/astonomicznego punktu widzenia - obiekty znajdujące się dalej mają bowiem wyższa wartość przesunięcia.  \n",
    "- Galaktyki wskazywany są dodatkowo przez wysokie wartości światła w okolicy czerwieni/podczerwieni. Jest to interesująca obserwacja, galaktyki są często obserwowane w tej części spektrum, jako iż dobrze przenika on przez pył międzygwiezdny, ale jednocześniej nie jest do końca jasne dlaczego najjaśniejsze obiekty powinny być klasyfikowane jako galaktyki, a nie gwiazdy.\n",
    "\n",
    "Nasze reguły okazały się najmniej skuteczne dla gwiazd, aby zrozumieć dlaczego posłużymy się jednak dalszą analizą - sprawdźmy jakie obiekty sa fałszywie wskazywane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b23f8b-f57e-4bb6-81f9-0eebad07a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rows = x_copy[(x_copy['medium_z']) & (x_copy['low_redshift']) & (x_copy['medium_i']) & (~x_copy['is_Star']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b627c-0de2-4b4f-a6cf-f70882709ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15c89e-7ac2-402f-aff7-09eab3c8f580",
   "metadata": {},
   "source": [
    "Wysnujemy hipotezę, iż fałszywa klasyfikacja wywołana jest tym, iż niektóre galaktyki znajdowały się na tyle blisko, iż zakwalifikowane zostały jako gwiazdy. Sprawdzimy to poprzez zmianę zakresów przesunięcia ku czerwieni na:\n",
    "\n",
    "- $x<0.01 $, niskie\n",
    "- $0.01<=x<1.5$, średnie\n",
    "- $  2<=x$, wysokie\n",
    "\n",
    "I powtórzenie operacji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519da155-49b0-489f-a197-630af5f1f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy = x.copy()\n",
    "x_copy.drop([\"Cluster\",'ra','dec'],axis=1,inplace=True)\n",
    "medium_upperbound = 22\n",
    "bright_upperbound = 17\n",
    "low_redshift_upperbound = 0.01\n",
    "medium_redshift_upperbound = 1.5\n",
    "\n",
    "for column in x_copy:   \n",
    "    if column!=\"redshift\":\n",
    "        name = \"dim_\"+column\n",
    "        x_copy[name] = x_copy[column] > medium_upperbound\n",
    "        name = \"medium_\"+column\n",
    "        x_copy[name] = (x_copy[column] <= medium_upperbound) &   (x_copy[column] > bright_upperbound)\n",
    "        name= \"bright_\"+column\n",
    "        x_copy[name] =x_copy[column] <= bright_upperbound\n",
    "    else:\n",
    "        name=\"low_\"+column\n",
    "        x_copy[name]=x_copy[column]<low_redshift_upperbound\n",
    "        name=\"medium_\"+column\n",
    "        x_copy[name]=(x_copy[column]>=low_redshift_upperbound) & (x_copy[column]<medium_redshift_upperbound)\n",
    "        name=\"high_\"+column\n",
    "        x_copy[name]=x_copy[column]>=medium_redshift_upperbound\n",
    "df_encoded = df.copy()\n",
    "x_copy['class'] = pd.DataFrame(y)\n",
    "x_copy.drop(features,axis=1,inplace=True)\n",
    "\n",
    "x_copy['is_Star']=x_copy['class']=='STAR'\n",
    "x_copy['is_Galaxy']=x_copy['class']=='GALAXY'\n",
    "x_copy['is_QSO']=x_copy['class']=='QSO'\n",
    "x_copy.drop('class',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# Używamy apriori do znalezienia częstych zbiorów\n",
    "frequent_itemsets = apriori(x_copy, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generowanie reguł asocjacyjnych\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.05,num_itemsets=len(x_copy))\n",
    "\n",
    "# Filtrujemy reguły, które odnoszą się do kolumny 'class'\n",
    "rules['consequents'] = rules['consequents'].apply(lambda x: list(x)[0])  # Zamieniamy krotki na elementy\n",
    "star_rules=rules[rules['consequents']=='is_Star']\n",
    "galaxy_rules =rules[rules['consequents']=='is_Galaxy']\n",
    "qso_rules =rules[rules['consequents']=='is_QSO']\n",
    "\n",
    "print(star_rules.loc[star_rules['confidence'].idxmax()]['antecedents'])\n",
    "star_rules['confidence'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663125b2-7b24-4019-aed8-69d756435b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(galaxy_rules.loc[galaxy_rules['confidence'].idxmax()]['antecedents'])\n",
    "galaxy_rules['confidence'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969faa1-c465-4bce-a064-a5010b2e15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qso_rules.loc[qso_rules['confidence'].idxmax()]['antecedents'])\n",
    "qso_rules['confidence'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53386521-5001-4f37-99c1-98cdb76ce5d4",
   "metadata": {},
   "source": [
    "Skuteczność naszej reguły w rozpoznawaniu gwiazd znacznie się zwiększyła - poprawa zakresów z pewnością zadziała. Wpływ przesunięcia ku czerwini na skuteczność reguł zdaje się być niezwykle wysoki. W dalszym ciągu galaktyki zdają się być jasniejsze w czerwieniach niż gwiazdy. Na koniec sprawdzimy odpowiedzi na dwa pytania wynikające z naszych obserwacji:\n",
    "- Czy przesunięcie ku czerwieni wystarczy dla określania rodzaju obiektu\n",
    "- Czy jasne w czerwieni obiekty to galaktyki?\n",
    "\n",
    "Zacznijmy od przesunięcia ku czerwieni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c28c5-22db-40b0-961f-b33f09bdc572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_copy['class']=pd.DataFrame(y)\n",
    "x_copy['low_redshift'] = x_copy['low_redshift'].astype(int)\n",
    "x_copy['medium_redshift'] = x_copy['medium_redshift'].astype(int)\n",
    "x_copy['high_redshift'] = x_copy['high_redshift'].astype(int)\n",
    "x_copy.loc[x_copy['low_redshift'] == 1, 'redshift_category'] = 'Low'\n",
    "x_copy.loc[x_copy['medium_redshift'] == 1, 'redshift_category'] = 'Medium'\n",
    "x_copy.loc[x_copy['high_redshift'] == 1, 'redshift_category'] = 'High'\n",
    "\n",
    "\n",
    "contingency_table = pd.crosstab(x_copy['class'], x_copy['redshift_category'])\n",
    "contingency_percentage =  contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(contingency_percentage, annot=True, linewidths=0.5, fmt='f')\n",
    "plt.title(\"Contingency Table Heatmap: Class vs Redshift Categories\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Redshift Categories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2c0fe-1a16-4612-b5a1-eed636103f01",
   "metadata": {},
   "source": [
    "Wskazać możemy, iż niskie przesunięcie ku czerwieni praktycznie jednoznacznie wksazuje na gwiazdy, a wysokie na QSO. Sprawa nie jest jednak oczywista dla średniego przesunięcia. Nie popełnilibyśmy dużego błędu mówiąc, że wskazuje ono na galaktykę, ale chcą odróżnić galaktyki od QSO wynik byłby znaczący, ta obserwacja jest jednak również pokrywająca się ze współczesną astronomią - QSO to zasadniczo odmiana galaktyk.\n",
    "\n",
    "Przeanalizujmy teraz czerwienie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481b83c-497e-49ed-9ad0-78d9abc45649",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_copy['dim_r'] = x_copy['dim_r'].astype(int)\n",
    "x_copy['medium_r'] = x_copy['medium_r'].astype(int)\n",
    "x_copy['bright_r'] = x_copy['bright_r'].astype(int)\n",
    "x_copy.loc[x_copy['dim_r'] == 1, 'r_category'] = 'Dim'\n",
    "x_copy.loc[x_copy['medium_r'] == 1, 'r_category'] = 'Medium'\n",
    "x_copy.loc[x_copy['bright_r'] == 1, 'r_category'] = 'Bright'\n",
    "\n",
    "x_copy['dim_i'] = x_copy['dim_i'].astype(int)\n",
    "x_copy['medium_i'] = x_copy['medium_i'].astype(int)\n",
    "x_copy['bright_i'] = x_copy['bright_i'].astype(int)\n",
    "x_copy.loc[x_copy['dim_i'] == 1, 'i_category'] = 'Dim'\n",
    "x_copy.loc[x_copy['medium_i'] == 1, 'i_category'] = 'Medium'\n",
    "x_copy.loc[x_copy['bright_i'] == 1, 'i_category'] = 'Bright'\n",
    "\n",
    "x_copy['dim_z'] = x_copy['dim_z'].astype(int)\n",
    "x_copy['medium_z'] = x_copy['medium_z'].astype(int)\n",
    "x_copy['bright_z'] = x_copy['bright_z'].astype(int)\n",
    "x_copy.loc[x_copy['dim_z'] == 1, 'z_category'] = 'Dim'\n",
    "x_copy.loc[x_copy['medium_z'] == 1, 'z_category'] = 'Medium'\n",
    "x_copy.loc[x_copy['bright_z'] == 1, 'z_category'] = 'Bright'\n",
    "\n",
    "contingency_table_r = pd.crosstab(x_copy['class'], x_copy['r_category'])\n",
    "contingency_percentage_r =  contingency_table_r.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "contingency_table_i = pd.crosstab(x_copy['class'], x_copy['i_category'])\n",
    "contingency_percentage_i =  contingency_table_i.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "contingency_table_z = pd.crosstab(x_copy['class'], x_copy['z_category'])\n",
    "contingency_percentage_z =  contingency_table_z.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.subplot(2,2,1)\n",
    "sns.heatmap(contingency_percentage_r, annot=True, linewidths=0.5, fmt='f')\n",
    "plt.title(\"Class vs Red Light Categories\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Red Light Categories\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.heatmap(contingency_percentage_i, annot=True, linewidths=0.5, fmt='f')\n",
    "plt.title(\"Class vs Near Infrared Categories\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Near Infrared Categories\")\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.heatmap(contingency_percentage_z, annot=True, linewidths=0.5, fmt='f')\n",
    "plt.title(\"Class vs Infrared Categories\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.xlabel(\"Infrared Categories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad0013-6256-4dc2-9c46-74d73f5529b7",
   "metadata": {},
   "source": [
    "Zauważyć możemy, że obiekty o średniej jasności znajdują się praktycznie w każdej z grup, choć obiekty QSO najczęściej znajdują się w tej kategorii jasności, co znalazło odzwirciedlenie we wcześniej stworzonych regułach.\n",
    "\n",
    "Obiekty jasne rozkąłdają się równomiernie między galaktyk i gwiazdy natomiast blade są praktycznie nie widoczne - wskazywać może to na miejsce do poprawy jako iż kategoria 'Dim' zdaje sie nie być użyteczna, a mogłaby posłużyć do lepszego objasnienia pozostałych. Podczerwienie i bliska podczerwień wskazują jedynie nieznacznie w stronę galaktyk - ten element utworzonych reguł jest albo typowy dla naszego zbioru, albo jego powiązanie z innymi cechami wskauzje na galaktyki, sam z siebie jest jednak niewystarczający."
   ]
>>>>>>> wzorce-sekwencji
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
